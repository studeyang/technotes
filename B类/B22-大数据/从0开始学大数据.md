> 来自极客时间《从0开始学大数据》--李智慧

# 开篇词讲为什么说每个软件工程师都应该懂大数据技术

将来，数据会越来越成为公司的核心资产和主要竞争力，公司的业务展开和产品进化也越来越朝着如何利用好数据价值的方向发展。不懂大数据和机器学习，可能连最基本的产品逻辑和商业意图都搞不清楚。**如果只懂编程，工程师的生存空间会越来越窄，发展也会处处受限**。

# 预习01讲大数据技术发展史：大数据的前世今生

不管是学习某门技术，还是讨论某个事情，最好的方式一定不是一头扎到具体细节里，而是应该从时空的角度先了解它的来龙去脉，以及它为什么会演进成为现在的状态。当你深刻理解了这些前因后果之后，再去看现状，就会明朗很多，也能更直接地看到现状背后的本质。说实话，这对于我们理解技术、学习技术而言，同等重要。

今天我们常说的大数据技术，其实起源于 Google 在2004年前后发表的三篇论文，也就是我们经常听到的“三驾马车”，分别是分布式文件系统 GFS、大数据分布式计算框架 MapReduce 和 NoSQL 数据库系统 BigTable。

**批处理计算**

一般说来，像MapReduce、Spark这类计算框架处理的业务场景都被称作**批处理计算**，因为它们通常针对以“天”为单位产生的数据进行一次计算，然后得到需要的结果，这中间计算需要花费的时间大概是几十分钟甚至更长的时间。因为计算的数据是非在线得到的实时数据，而是历史数据，所以这类计算也被称为**大数据离线计算**。

而在大数据领域，还有另外一类应用场景，它们需要对实时产生的大量数据进行即时计算，比如对于遍布城市的监控摄像头进行人脸识别和嫌犯追踪。这类计算称为**大数据流计算**，相应地，有Storm、Flink、Spark Streaming等流计算框架来满足此类大数据应用的场景。 流式计算要处理的数据是实时在线产生的数据，所以这类计算也被称为**大数据实时计算**。

**大数据的应用场景**

**大数据处理的主要应用场景包括数据分析、数据挖掘与机器学习**。数据分析主要使用Hive、Spark SQL等SQL引擎完成；数据挖掘与机器学习则有专门的机器学习框架TensorFlow、Mahout以及MLlib等，内置了主要的机器学习和数据挖掘算法。

此外，大数据要存入分布式文件系统（HDFS），要有序调度MapReduce和Spark作业执行，并能把执行结果写入到各个应用系统的数据库中，还需要有一个**大数据平台**整合所有这些大数据组件和企业应用系统。

![image-20250626225305406](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202506262253485.png)

# 预习02讲大数据应用发展史：从搜索引擎到人工智能

我们对大数据技术的使用同样也经历了一个发展过程。

**大数据应用的搜索引擎时代**

作为全球最大的搜索引擎公司，Google也是我们公认的大数据鼻祖，它存储着全世界几乎所有可访问的网页，数目可能超过万亿规模，全部存储起来大约需要数万块磁盘。为了将这些文件存储起来，Google开发了GFS（Google文件系统），**将数千台服务器上的数万块磁盘统一管理起来，然后当作一个文件系统，统一存储所有这些网页文件**。

你可能会觉得，如果只是简单地将所有网页存储起来，好像也没什么太了不起的。没错，但是Google得到这些网页文件是要构建搜索引擎，需要对所有文件中的单词进行词频统计，然后根据PageRank算法计算网页排名。这中间，Google需要对这数万块磁盘上的文件进行计算处理，这听上去就很了不起了吧。当然，也正是基于这些需求，Google又开发了MapReduce大数据计算框架。

**大数据应用的数据仓库时代**

曾经我们在进行数据分析与统计时，仅仅局限于数据库，在数据库的计算环境中对数据库中的数据表进行统计分析。并且受数据量和计算能力的限制，我们只能对最重要的数据进行统计和分析。这里所谓最重要的数据，通常指的都是给老板看的数据和财务相关的数据。

而Hive可以在Hadoop上进行SQL操作，实现数据统计与分析。也就是说，**我们可以用更低廉的价格获得比以往多得多的数据存储与计算能力**。我们可以把运行日志、应用采集数据、数据库数据放到一起进行计算分析，获得以前无法得到的数据结果，企业的数据仓库也随之呈指数级膨胀。

**大数据应用的数据挖掘时代**

讲个真实的案例，很早以前商家就通过数据发现，买尿不湿的人通常也会买啤酒，于是精明的商家就把这两样商品放在一起，以促进销售。啤酒和尿不湿的关系，你可以有各种解读，但是如果不是通过数据挖掘，可能打破脑袋也想不出它们之间会有关系。在商业环境中，如何解读这种关系并不重要，重要的是它们之间只要存在关联，就可以进行**关联分析**，最终目的是让用户尽可能看到想购买的商品。

除了商品和商品有关系，还可以利用人和人之间的关系推荐商品。如果两个人购买的商品有很多都是类似甚至相同的，不管这两个人天南海北相隔多远，他们一定有某种关系，比如可能有差不多的教育背景、经济收入、兴趣爱好。根据这种关系，可以进行关联推荐，让他们看到自己感兴趣的商品。

**大数据应用的机器学习时代**

在过去，我们受数据采集、存储、计算能力的限制，只能通过抽样的方式获取小部分数据，无法得到完整的、全局的、细节的规律。**而现在有了大数据，可以把全部的历史数据都收集起来，统计其规律，进而预测正在发生的事情**。

这就是机器学习。

将人类活动产生的数据，通过机器学习得到统计规律，进而可以模拟人的行为，使机器表现出人类特有的智能，这就是人工智能AI。

# 预习03讲大数据应用领域：数据驱动一切

**大数据在医疗健康领域的应用**

1. 医学影像智能识别

比如X光片里的异常病灶位置，是可以通过机器学习智能识别出来的。甚至可以说医学影像智能识别在某些方面已经比一般医生拥有更高的读图和识别能力，但是鉴于医疗的严肃性，现在还很少有临床方面的实践。

2. 病历大数据智能诊疗

利用大数据技术将这些病历进行处理、分析、统计、 挖掘，可以构成一个病历知识库，可以分享给更多人，即构成一个智能辅助诊疗系统。 针对同类疾病和其他上下文信息（化验结果、病史、年龄性别、病人回访信息等），可以挖掘出哪种治疗手段可以用更低的治疗成本、更少的病人痛苦，获得更好的治疗效果。

**大数据在教育领域的应用**

1. AI外语老师

得益于语音识别和语音合成技术的成熟，一些在线教育网站尝试用人工智能外语老师进行外语教学。这里面的原理其实并不复杂，聊天机器人技术已经普遍应用，只要将学习的知识点设计进聊天的过程中，就可以实现一个简单的AI外语老师了。

2. 智能解题

比较简单的智能解题系统其实是利用搜索引擎技术，在收集大量的试题以及答案的基础上，进行试题匹配，将匹配成功的答案返回。

进阶一点的智能解题系统，通过图像识别与自然语言处理，进行相似性匹配。更改试题的部分数字、文字表述，但是不影响实质性解答思路，依然可以解答。

高阶的智能解题系统，利用神经网络机器学习技术，将试题的自然语言描述转化成形式语言，然后分析知识点和解题策略，进行自动推导，从而完成实质性的解题。

**大数据在社交媒体领域的应用**

我们日常在各种互联网应用和社交媒体上发表各种言论，这些言论事实上反映了最准确的民情舆论。一个个体的言论基本没有意义，但是大量的、全国乃至全球的言论数据表现出的统计特性，就有了非常重要的意义。

**大数据在金融领域的应用**

在金融借贷中，如何识别出高风险用户，要求其提供更多抵押、支付更高利息、调整更低的额度，甚至拒绝贷款，从而降低金融机构的风险？

事实上，金融行业已经沉淀了大量的历史数据，利用这些数据进行计算，可以得到用户特征和风险指数的曲线（即风控模型）。当新用户申请贷款的时候，将该用户特征带入曲线进行计算，就可以得到该用户的风险指数，进而自动给出该用户的贷款策略。

**大数据在新零售领域的应用**

亚马逊Go无人店使用大量的摄像头，实时捕捉用户行为，判断用户取出还是放回商品、取了何种商品等。这实际上是大数据流计算与机器学习的结合，最终实现的购物效果是，无需排队买单，进去就拿东西，拿好了就走。

**大数据在交通领域的应用**

现在几乎所有的城市路段、交通要点都有不止一个监控摄像头在实时采集数据，这些数据一方面可以用于公共安全，比如近年来一些警匪片里会有一些场景：犯罪嫌疑人驾车出逃，警方只要定位了车辆，不管它到哪里，系统都可以自动调出相应的摄像头，实时看到现场画面。

此外，各种导航软件也在不停采集数据，通过分析用户当前位置和移动速度，判断道路拥堵状态，并实时修改推荐的导航路径。

还有就是无人驾驶技术，无人驾驶就是在人的驾驶过程中实时采集车辆周边数据和驾驶控制信息，然后通过机器学习，获得周边信息与驾驶方式的对应关系（自动驾驶模型）。然后将这个模型应用到无人驾驶汽车上，传感器获得车辆周边数据后，就可以通过自动驾驶模型计算出车辆控制信息（转向、刹车等）。

# 04讲移动计算比移动数据更划算

传统的软件计算处理模型，都是“输入 -> 计算 -> 输出”模型。但是在互联网大数据时代，需要计算处理的数据量急速膨胀，传统的计算处理模型已经不能适用于大数据时代的计算要求。

你能想象一个程序读取PB级的数据进行计算是怎样一个场景吗？一个程序所能调度的网络带宽（通常数百MB）、内存容量（通常几十GB ）、磁盘大小（通常数TB）、CPU运算速度是不可能满足这种计算要求的。

**那么如何解决PB级数据进行计算的问题呢？**

这个问题的解决思路其实跟大型网站的分布式架构思路是一样的，采用分布式集群的解决方案，用数千台甚至上万台计算机构建一个大数据计算处理集群，利用更多的网络带宽、内存空间、磁盘容量、CPU核心数去进行计算处理。

大数据计算处理通常针对的是网站的存量数据，将这些统计规律和关联关系计算出来，并由此进一步改善网站的用户体验和运营决策。

为了解决这种计算场景的问题，技术专家们设计了一套相应的技术架构方案。这套方案的核心思路是，既然数据是庞大的，而程序要比数据小得多，将数据输入给程序是不划算的，那么就反其道而行之，将程序分发到数据所在的地方进行计算，也就是所谓的移动计算比移动数据更划算。

**移动计算程序到数据所在位置进行计算是如何实现的呢？**

1、将数据存储在服务器上。

将待处理的大规模数据存储在服务器集群上，主要使用HDFS分布式文件存储系统，将文件分成很多块（Block），以块为单位存储在集群的服务器上。

2、启动分布式任务。

数据引擎根据集群里不同服务器的计算能力，在每台服务器上启动若干任务进程，这些进程会等待数据引擎给它们分配执行任务。

3、编程并打包。

使用大数据计算框架支持的编程模型进行编程，比如 Hadoop 的 MapReduce 编程模型，或者 Spark 的 RDD 编程模型。应用程序编写好以后，将其打包，MapReduce 和 Spark 都是在 JVM 环境中运行，所以打包出来的是一个 Java 的 JAR 包。

4、执行应用程序 JAR 包。

用 Hadoop 或者 Spark 的启动命令执行这个应用程序的 JAR 包，首先执行引擎会解析程序要处理的数据输入路径，根据输入数据量的大小，将数据分成若干片（Split），每一个数据片都分配给一个任务执行进程去处理。

5、加载任务程序包。

任务执行进程收到分配的任务后，检查自己是否有任务对应的程序包，如果没有就去下载程序包，下载以后通过反射的方式加载程序。走到这里，最重要的一步，也就是移动计算就完成了。

6、读取数据并执行计算。

加载程序后，任务执行进程根据分配的数据片的文件地址和数据在文件内的偏移量读取数据，并把数据输入给应用程序相应的方法去执行，从而实现在分布式服务器集群中移动计算程序，对大规模数据进行并行处理的计算目标。

这只是大数据计算实现过程的简单描述，具体过程我们会在讲到 HDFS、MapReduce 和 Spark 的时候详细讨论。

# 05讲从RAID看垂直伸缩到水平伸缩的演化

我们要想对数据进行计算，首先要解决的其实是大规模数据的存储问题。如果一个文件的大小超过了一张磁盘的大小，你该如何存储？

大规模数据存储都需要解决以下三个问题：

- 数据存储容量的问题。大数据要解决的是数以PB计的数据计算问题，而一般的服务器磁盘容量通常1～2TB，那么如何存储这么大规模的数据呢？
- 数据读写速度的问题。一般磁盘的连续读写速度为几十MB，以这样的速度，几十PB的数据恐怕要读写到天荒地老。
- 数据可靠性的问题。磁盘大约是计算机设备中最易损坏的硬件了，通常情况一块磁盘使用寿命大概是一年，如果磁盘损坏了，数据怎么办？

**RAID（独立磁盘冗余阵列）**

RAID（独立磁盘冗余阵列）技术是将多块普通磁盘组成一个阵列，共同对外提供服务，改善磁盘的存储容量、读写速度，增强磁盘的可用性和容错能力。

常用RAID技术有图中下面这几种。

![image-20250703234619734](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507032346779.png)

首先，我们先假设服务器有N块磁盘。**RAID 0**是数据在从内存缓冲区写入磁盘时，根据磁盘数量将数据分成N份，这些数据同时并发写入N块磁盘，使得数据整体写入速度是一块磁盘的N倍；读取的时候也一样，因此RAID 0具有极快的数据读写速度。但是RAID 0不做数据备份，N块磁盘中只要有一块损坏，数据完整性就被破坏，其他磁盘的数据也都无法使用了。

**RAID 1**是数据在写入磁盘时，将一份数据同时写入两块磁盘，这样任何一块磁盘损坏都不会导致数据丢失，插入一块新磁盘就可以通过复制数据的方式自动修复，具有极高的可靠性。

结合RAID 0和RAID 1两种方案构成了**RAID 10**，它是将所有磁盘N平均分成两份，数据同时在两份磁盘写入，相当于RAID 1；但是平分成两份，在每一份磁盘（也就是N/2块磁盘）里面，利用RAID 0技术并发读写，这样既提高可靠性又改善性能。不过RAID 10的磁盘利用率较低，有一半的磁盘用来写备份数据。

顺着这个思路，**RAID 3**可以在数据写入磁盘的时候，将数据分成N-1份，并发写入N-1块磁盘，并在第N块磁盘记录校验数据。但是在数据修改较多的场景中，任何磁盘数据的修改，都会导致第N块磁盘重写校验数据。频繁写入的后果是第N块磁盘比其他磁盘更容易损坏，需要频繁更换，所以RAID 3很少在实践中使用，因此在上面图中也就没有单独列出。

相比RAID 3，**RAID 5**是使用更多的方案。RAID 5和RAID 3很相似，但是校验数据不是写入第N块磁盘，而是螺旋式地写入所有磁盘中。这样校验数据的修改也被平均到所有磁盘上，避免RAID 3频繁写坏一块磁盘的情况。

RAID 6和RAID 5类似，但是数据只写入N-2块磁盘，并螺旋式地在两块磁盘中写入校验信息（使用不同算法生成）。

从下面表格中你可以看到在相同磁盘数目（N）的情况下，各种RAID技术的比较。

![image-20250703235428214](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507032354256.png)

**RAID是如何解决存储的三个关键问题的？**

现在我来总结一下，看看RAID是如何解决我一开始提出的，关于存储的三个关键问题。

- 数据存储容量的问题。RAID使用了N块磁盘构成一个存储阵列，如果使用RAID 5，数据就可以存储在N-1块磁盘上，这样将存储空间扩大了N-1倍。
- 数据读写速度的问题。RAID根据可以使用的磁盘数量，将待写入的数据分成多片，并发同时向多块磁盘进行写入，显然写入的速度可以得到明显提高；同理，读取速度也可以得到明显提高。
- 数据可靠性的问题。使用RAID 10、RAID 5或者RAID 6方案的时候，由于数据有冗余存储，或者存储校验信息，所以当某块磁盘损坏的时候，可以通过其他磁盘上的数据和校验数据将丢失磁盘上的数据还原。

**垂直伸缩 VS 水平伸缩**

在计算机发展的早期，我们获得更强大计算能力的手段主要依靠垂直伸缩。但是到了互联网时代，这种垂直伸缩的路子走不通了。

RAID可以看作是一种垂直伸缩，一台计算机集成更多的磁盘实现数据更大规模、更安全可靠的存储以及更快的访问速度。而HDFS则是水平伸缩，通过添加更多的服务器实现数据更大、更快、更安全存储与访问。

RAID技术只是在单台服务器的多块磁盘上组成阵列，大数据需要更大规模的存储空间和更快的访问速度。将RAID思想原理应用到分布式服务器集群上，就形成了Hadoop分布式文件系统HDFS的架构思想。

# 06讲新技术层出不穷，HDFS依然是存储的王者

我们知道，Google 大数据“三驾马车”的第一驾是 GFS（Google 文件系统），以说分布式文件存储是分布式计算的基础。

HDFS作为最早的大数据存储系统，存储着宝贵的数据资产，HDFS也许不是最好的大数据存储技术，但依然最重要的大数据存储技术。今天我们来聊聊HDFS是如何实现大数据高速、可靠的存储和访问的。

**HDFS是如何实现大数据高速访问的？**

HDFS可以部署在一个比较大的服务器集群上，集群中所有服务器的磁盘都可供HDFS使用，所以整个HDFS的存储空间可以达到PB级容量。

![image-20250703233250112](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507032332206.png)

从上图中你可以看到HDFS的关键组件有两个，一个是DataNode，一个是NameNode。

DataNode负责文件数据的存储和读写操作，HDFS将文件数据分割成若干数据块（Block），每个DataNode存储一部分数据块，这样文件就分布存储在整个HDFS服务器集群中。应用程序客户端（Client）可以并行对这些数据块进行访问，极大地提高了访问速度。

**HDFS是如何实现大数据可靠存储的？**

NameNode负责整个分布式文件系统的元数据（MetaData）管理，也就是文件路径名、数据块的ID以及存储位置等信息。HDFS为了保证数据的高可用，会将一个数据块（Block）复制为多份（默认为3份），并将多份相同的数据块存储在不同的服务器（DataNodes）上。这样当有磁盘损坏，或者某个DataNode服务器宕机，客户端会查找其备份的数据块进行访问。

下面这张图是数据块多份复制存储的示意，图中对于文件/users/sameerp/data/part-0，其复制备份数设置为2，存储的BlockID分别为1、3。任何一台服务器宕机后，每个数据块都至少还有一个备份存在，不会影响对文件/users/sameerp/data/part-0的访问。

![image-20250704232012685](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507042320780.png)

HDFS是如何保证存储的高可用性呢？我们尝试从不同层面来讨论一下HDFS的高可用设计。

1. 数据存储错乱

磁盘介质在存储过程中受环境影响，数据可能会出现错乱。HDFS的应对措施是，对于存储在DataNode上的数据块，计算并存储校验和（CheckSum），在读取数据的时候，重新计算并读取校验和，如果不一致就抛出异常，应用程序捕获异常后就到其他DataNode上读取备份数据。

2. 磁盘故障

如果DataNode监测到本机的某块磁盘损坏，就将该块磁盘上存储的所有BlockID报告给NameNode，NameNode检查这些数据块还在哪些DataNode上有备份，通知相应的DataNode服务器将对应的数据块复制到其他服务器上，以保证数据块的备份数满足要求。

3. DataNode故障

DataNode会通过心跳和NameNode保持通信，如果DataNode超时未发送心跳，NameNode就会认为这个DataNode已经宕机失效，立即查找其它服务器上的数据块，随后通知这些服务器再复制一份数据块到其他服务器上，保证数据块备份数符合用户设置的数目。

4. NameNode故障

NameNode是整个HDFS的核心，如果NameNode上记录的数据丢失，整个集群所有DataNode存储的数据也就没用了。

所以，NameNode高可用容错能力非常重要。NameNode采用主从热备的方式提供高可用服务，请看下图。

![image-20250703233725080](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507032337129.png)

集群部署两台NameNode服务器，一台作为主服务器提供服务，一台作为从服务器进行热备，正常运行期间，主从NameNode之间通过一个共享存储系统shared edits来同步文件系统的元数据信息。当主NameNode服务器宕机，从NameNode会通过ZooKeeper升级成为主服务器。

DataNode会同时向两个NameNode发送心跳数据，但是只有主NameNode才能向DataNode返回控制信息。

# 07讲为什么说MapReduce既是编程模型又是计算框架

今天我们来聊聊Hadoop解决大规模数据分布式计算的方案——MapReduce。

MapReduce既是一个编程模型，又是一个计算框架。也就是说，开发人员必须基于MapReduce编程模型进行编程开发，然后将程序通过MapReduce计算框架分发到Hadoop集群中运行。我们先看一下作为编程模型的MapReduce。

**MapReduce编程模型**

MapReduce是一种非常简单又非常强大的编程模型。

简单在于其编程模型只包含Map和Reduce两个过程，map的主要输入是一对<Key, Value>值，经过map计算后输出一对<Key, Value>值；然后将相同Key合并，形成<Key, Value集合>；再将这个<Key, Value集合>输入reduce，经过计算输出零个或多个<Key, Value>对。

同时，MapReduce又是非常强大的，不管是关系代数运算（SQL计算），还是矩阵运算（图计算），大数据领域几乎所有的计算需求都可以通过MapReduce编程来实现。

下面，我以WordCount程序为例，一起来看下MapReduce的计算过程。

**MapReduce计算过程**

WordCount主要解决的是文本处理中词频统计的问题，这个统计过程你可以看下面这张图。

![image-20250707230135641](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507072301689.png)

如果用Python语言，单机处理WordCount的代码是这样的。

```python
# 文本前期处理
strl_ist = str.replace('\n', '').lower().split(' ')
count_dict = {}
# 如果字典里有该单词则加1，否则添加入字典
for str in strl_ist:
if str in count_dict.keys():
    count_dict[str] = count_dict[str] + 1
    else:
        count_dict[str] = 1
```

小数据量用单机统计词频很简单，但是如果想统计全世界互联网所有网页（数万亿计）的词频数（而这正是Google这样的搜索引擎的典型需求），不可能写一个程序把全世界的网页都读入内存，这时候就需要用MapReduce编程来解决。

```java
public class WordCount {

  public static class TokenizerMapper
      extends Mapper<Object, Text, Text, IntWritable> {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    //Key在一般计算中都不会用到，Value是要统计的所有文本中的一行数据
    public void map(Object key, Text value, Context context)
        throws IOException, InterruptedException {
      //map函数的计算过程是，将这行文本中的单词提取出来
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        //针对每个单词输出一个<word, 1>这样的<Key, Value>对
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }
    
  //MapReduce计算框架会将这些<word , 1>收集起来，将相同的word放在一起，
  //形成<word, <1,1,1,1,1,1,1…>>这样的<Key, Value集合>数据，
  //然后将其输入给reduce函数。
    
  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    //Key就是具体的单词word，Values就是由很多个1组成的集合
    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context)
        throws IOException, InterruptedException {
      //将这个集合里的1求和
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      //再将单词（word）和这个和（sum）组成一个<Key, Value>，也就是<word, sum>输出
      result.set(sum);
      context.write(key, result);
    }
  }
}
```

一个map函数可以针对一部分数据进行运算，这样就可以将一个大数据切分成很多块（这也正是HDFS所做的），MapReduce计算框架为每个数据块分配一个map函数去计算，从而实现大数据的分布式计算。

假设有两个数据块的文本数据需要进行词频统计，MapReduce计算过程如下图所示。

![image-20250707231639713](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507072316771.png)

以上就是MapReduce编程模型的主要计算过程和原理。但是这样一个MapReduce程序要想在分布式环境中执行，还需要一个计算框架来调度执行它，而这个计算框架也叫MapReduce。

# 08讲MapReduce如何让数据完成一次旅行

今天我们来讨论MapReduce如何让数据完成一次旅行，也就是MapReduce计算框架是如何运作的。

在实践中，这个过程有两个关键问题需要处理。

- 计算代码是如何发送到数据块所在服务器的？启动以后如何知道自己需要计算的数据在文件什么位置（BlockID是什么）？
- 不同服务器的map输出的<Key, Value> ，如何把相同的Key聚合在一起发送给Reduce任务进行处理？

这两个关键问题对应的就是下图中的两处“MapReduce框架处理”，具体来说，它们分别是MapReduce作业启动和运行，以及MapReduce数据合并与连接。

![image-20250707224211499](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507072242598.png)

**MapReduce作业启动和运行机制**

MapReduce运行过程涉及三类关键进程。

1. 大数据应用进程。这是由用户启动的MapReduce程序进程，这类进程是启动MapReduce程序的主入口，主要是指定Map和Reduce类、输入输出文件路径等，并提交作业给Hadoop集群。
2. JobTracker进程。这类进程根据要处理的输入数据量，命令TaskTracker进程启动相应数量的Map和Reduce进程任务，并管理整个作业生命周期的任务调度和监控。JobTracker进程在整个Hadoop集群全局唯一。
3. TaskTracker进程。这个进程负责启动和管理Map进程以及Reduce进程。因为需要每个数据块都有对应的map函数，Hadoop集群中绝大多数服务器同时运行DataNode进程和TaskTracker进程。

JobTracker进程和TaskTracker进程是主从关系，主服务器通常只有一台，从服务器可能有几百上千台，所有的从服务器听从主服务器的控制和调度安排。

如果我们把这个计算过程看作一次小小的旅行，这个旅程可以概括如下：

![image-20250707224444518](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507072244564.png)

1. 应用进程JobClient将用户作业JAR包存储在HDFS中，将来这些JAR包会分发给Hadoop集群中的服务器执行MapReduce计算。
2. 应用程序提交job作业给JobTracker。
3. JobTracker根据作业调度策略创建JobInProcess树，每个作业都会有一个自己的JobInProcess树。
4. JobInProcess根据输入数据分片数目（通常情况就是数据块的数目）和设置的Reduce数目创建相应数量的TaskInProcess。
5. TaskTracker进程和JobTracker进程进行定时通信。
6. 如果TaskTracker有空闲的计算资源（有空闲CPU），JobTracker就会给它分配任务，并且匹配相同机器上的数据块计算任务给它。
7. TaskTracker收到任务后根据任务类型（是Map还是Reduce）和任务参数（作业JAR包路径、输入数据文件路径、要处理的数据在文件中的起始位置和偏移量、数据块多个备份的DataNode主机名等），启动相应的Map或者Reduce进程。
8. Map或者Reduce进程启动后，检查本地是否有要执行任务的JAR包文件，如果没有，就去HDFS上下载，然后加载Map或者Reduce代码开始执行。
9. 如果是Map进程，从HDFS读取数据（通常要读取的数据块正好存储在本机）；如果是Reduce进程，将结果数据写出到HDFS。

其实，你要做的仅仅是编写一个map函数和一个reduce函数就可以了，根本不用关心这两个函数是如何被分布启动到集群上的，也不用关心数据块又是如何分配给计算任务的。这一切都由MapReduce计算框架完成！

**MapReduce数据合并与连接机制**

在map输出与reduce输入之间，MapReduce计算框架处理数据合并与连接操作，这个操作有个专门的词汇叫shuffle。那到底什么是shuffle？shuffle的具体过程又是怎样的呢？请看下图。

![image-20250707224838253](https://technotes.oss-cn-shenzhen.aliyuncs.com/2024/202507072248297.png)

在Map任务快要计算完成的时候，MapReduce计算框架会启动shuffle过程，在Map任务进程调用一个Partitioner接口（对Map产生的每个<Key, Value>进行Reduce分区选择），然后通过HTTP通信发送给对应的Reduce进程。Reduce任务进程对收到的<Key, Value>进行排序和合并，相同的Key放在一起，组成一个<Key, Value集合>传递给Reduce执行。

map输出的<Key, Value>shuffle到哪个Reduce进程是这里的关键，它是由Partitioner来实现，MapReduce框架默认的Partitioner用Key的哈希值对Reduce任务数量取模，相同的Key一定会落在相同的Reduce任务ID上。从实现上来看的话，这样的Partitioner代码只需要一行。

```java
/** Use {@link Object#hashCode()} to partition. */ 
public int getPartition(K2 key, V2 value, int numReduceTasks) {
    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
}
```

其实对shuffle的理解，你只需要记住这一点：分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是shuffle。

# 09讲为什么我们管Yarn叫作资源调度框架













